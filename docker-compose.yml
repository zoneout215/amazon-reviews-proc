version: '3.8'

x-trg-env2: &trg-env2
  CLICKHOUSE_HOST: "clickhouse-server"
  CLICKHOUSE_DB: default
  CLICKHOUSE_USER: default
  CLICKHOUSE_PASSWORD: ''
  CLICKHOUSE_PORT: 8123 

x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
  environment: &airflow-environment
    AIRFLOW__CORE__REMOTE_LOGGING: 'False'
    AIRFLOW__CORE__BASE_LOG_FOLDER: '/opt/airflow/logs'
    AIRFLOW__CORE__DAGS_FOLDER: '/opt/airflow/dags'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8080
    HDFS_URI: 'http://namenode:9870'
    <<: *trg-env2
  env_file:
    - local.env
  restart: always
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/plugins:/opt/airflow/plugins
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/source_data:/opt/airflow/source_data
    - ./dbt:/opt/airflow/dbt
    - ./dbt/profiles.yml:/home/airflow/.dbt
    - ./migrations:/opt/airflow/migrations

# services:
#   object_store_namenode:
#     image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
#     container_name: namenode
#     restart: always
#     ports:
#       - "9870:9870"
#       - "9001:9000"
#     expose:
#       - "9001"
#     volumes:
#       - ./hadoop/namenode:/hadoop/dfs/name
#     environment:
#       CLUSTER_NAME: 'test'
#     env_file:
#       - ./hadoop/hadoop.env

#   object_store_datanode_1:
#     image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
#     container_name: datanode1
#     restart: always
#     ports:
#       - "9864:9864"
#     volumes:
#       - ./hadoop/datanode1:/hadoop/dfs/data
#     environment:
#       SERVICE_PRECONDITION: 'namenode:9870'
#     env_file:
#       - ./hadoop/hadoop.env

#   object_store_datanode_2:
#     image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
#     container_name: datanode2
#     restart: always
#     ports:
#       - "9865:9864"
#     volumes:
#       - ./hadoop/datanode2:/hadoop/dfs/data
#     environment:
#       SERVICE_PRECONDITION: 'namenode:9870'
#     env_file:
#       - ./hadoop/hadoop.env

  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    restart: always
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    environment:
      <<: *airflow-environment
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__CORE__DAG_CONCURRENCY: 8
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      - postgres
    command: webserver

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    environment:
      <<: *airflow-environment
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      - airflow-webserver
    command: scheduler

  # clickhouse-server:
  #   image: clickhouse/clickhouse-server:24.10.4
  #   container_name: clickhouse-server
  #   hostname: clickhouse-server
  #   ports:
  #     - "8123:8123" # HTTP port
  #     - "9000:9000" # Native TCP port
  #   expose:
  #     - "9000"
  #   environment: 
  #     <<: *trg-env2
  #   #   CLICKHOUSE_DB: default
  #   #   CLICKHOUSE_USER: clickhouse
  #   #   CLICKHOUSE_PASSWORD: ''
  #   #   CLICKHOUSE_PORT: 9000 
  #   #   CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    
  #   volumes:
  #     - ./clickhouse/clickhouse_data:/var/lib/clickhouse # Persist data
  #   ulimits:
  #     nofile: 262144
  #   deploy: 
  #     resources:
  #       reservations:
  #         cpus: '4.0'
